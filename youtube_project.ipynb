{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bfaa74d-ab7c-42ae-815b-c26d78f4fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "import json\n",
    "from googleapiclient.discovery import build\n",
    "from google.oauth2.credentials import Credentials\n",
    "import google.auth\n",
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import json\n",
    "import gradio as gr\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "773b8f38-310f-4bb4-a62f-b47457c19c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def chunk_text_using_langchain(text, chunk_size=500):\n",
    "    \"\"\"Splits text into smaller chunks using LangChain's CharacterTextSplitter.\"\"\"\n",
    "    splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=50)\n",
    "    chunks = splitter.split_text(text)  # Split the text into chunks\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fae02e3e-bb19-4e06-b109-dcd56aaf4e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_liked_videos(youtube):\n",
    "    liked_videos = []\n",
    "    request = youtube.videos().list(\n",
    "        part=\"snippet,contentDetails,statistics\",\n",
    "        myRating=\"like\",\n",
    "        maxResults=199  # YouTube API limits to 50 per request\n",
    "    )\n",
    "    \n",
    "    while request and len(liked_videos) < 200:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            liked_videos.append({\n",
    "                \"title\": item[\"snippet\"][\"title\"],\n",
    "                \"duration\": item[\"contentDetails\"][\"duration\"],\n",
    "                \"viewcount\": item[\"statistics\"].get(\"viewCount\", \"0\"),\n",
    "                \"categoryid\": item[\"snippet\"][\"categoryId\"],\n",
    "                \"id\": item[\"id\"],\n",
    "                \"description\": item[\"snippet\"].get(\"description\", \"\"),\n",
    "                \"channelid\": item[\"snippet\"][\"channelId\"],\n",
    "                \"tags\": item[\"snippet\"].get(\"tags\", [])\n",
    "            })\n",
    "        request = youtube.videos().list_next(request, response)\n",
    "    \n",
    "    return liked_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de99b039-0e73-47b2-be88-3dd6a5052725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subscriptions(youtube):\n",
    "    subscriptions = []\n",
    "    request = youtube.subscriptions().list(\n",
    "        part=\"snippet\",\n",
    "        mine=True,\n",
    "        maxResults=50\n",
    "    )\n",
    "\n",
    "    while request:\n",
    "        response = request.execute()\n",
    "        for item in response.get(\"items\", []):\n",
    "            subscriptions.append({\n",
    "                \"id\": item[\"snippet\"][\"resourceId\"][\"channelId\"],\n",
    "                \"title\": item[\"snippet\"][\"title\"],\n",
    "                \"description\": item[\"snippet\"].get(\"description\", \"\")\n",
    "            })\n",
    "        request = youtube.subscriptions().list_next(request, response)\n",
    "\n",
    "    return subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49bc126d-ca0c-4005-acb3-c8a100dda8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uploads(youtube):\n",
    "    request = youtube.channels().list(\n",
    "        part=\"statistics\",\n",
    "        mine=True\n",
    "    )\n",
    "    response = request.execute()\n",
    "    total_uploads = response[\"items\"][0][\"statistics\"].get(\"videoCount\", \"0\")\n",
    "    return {\"length of total uploads\": total_uploads}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e85419b9-d1de-4f7b-858e-88d30dcb175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_chromadb(data):\n",
    "    # Load environment variables and OpenAI API key\n",
    "    load_dotenv()\n",
    "    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "    # Initialize ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "    collection = chroma_client.get_or_create_collection(name=\"youtube_data\")\n",
    "    \n",
    "    print(\"‚úÖ OpenAI API Key Loaded and ChromaDB Initialized\")\n",
    "\n",
    "    # Function to get OpenAI embedding using the new API\n",
    "    def get_openai_embedding(text):\n",
    "        \"\"\"Generate an embedding using OpenAI's text-embedding-ada-002 model.\"\"\"\n",
    "        response = openai.embeddings.create(\n",
    "            input=text,\n",
    "            model=\"text-embedding-ada-002\"\n",
    "        )\n",
    "        # Correct way to access the embedding data\n",
    "        return response.data[0].embedding\n",
    "\n",
    "    # Store Liked Videos\n",
    "    for video in data[\"liked_videos\"]:\n",
    "        # Chunk the description (assuming chunk_text_using_langchain is defined elsewhere)\n",
    "        description_chunks = chunk_text_using_langchain(video[\"description\"])\n",
    "\n",
    "        for i, chunk in enumerate(description_chunks):\n",
    "            embedding = get_openai_embedding(chunk)\n",
    "\n",
    "            # Convert tags list into a string (separated by commas)\n",
    "            tags_str = \", \".join(video[\"tags\"]) if isinstance(video[\"tags\"], list) else str(video[\"tags\"])\n",
    "\n",
    "            collection.add(\n",
    "                ids=[f\"liked_{video['id']}_chunk_{i}\"],\n",
    "                embeddings=[embedding],\n",
    "                metadatas=[{\n",
    "                    \"type\": \"liked_video\",\n",
    "                    \"title\": video[\"title\"],\n",
    "                    \"channel\": video[\"channelid\"],\n",
    "                    \"description_chunk\": chunk,\n",
    "                    \"tags\": tags_str,  # Converted list to string\n",
    "                    \"category_id\": video[\"categoryid\"],\n",
    "                    \"view_count\": video[\"viewcount\"],\n",
    "                    \"duration\": video[\"duration\"]\n",
    "                }]\n",
    "            )\n",
    "\n",
    "    # Store Subscriptions\n",
    "    for sub in data[\"subscriptions\"]:\n",
    "        embedding = get_openai_embedding(sub[\"title\"] + \" \" + sub[\"description\"])\n",
    "\n",
    "        collection.add(\n",
    "            ids=[f\"sub_{sub['id']}\"],\n",
    "            embeddings=[embedding],\n",
    "            metadatas=[{\n",
    "                \"type\": \"subscription\",\n",
    "                \"title\": sub[\"title\"],\n",
    "                \"description\": sub[\"description\"]\n",
    "            }]\n",
    "        )\n",
    "\n",
    "    # Store Total Uploads\n",
    "    total_uploads_count = len(data[\"uploads\"])  # Calculate the total number of uploads\n",
    "\n",
    "    collection.add(\n",
    "        ids=[\"total_uploads\"],\n",
    "        embeddings=[[0] * 1536],  # Placeholder vector\n",
    "        metadatas=[{\n",
    "            \"type\": \"total_uploads\",\n",
    "            \"count\": total_uploads_count  # Correctly using the number of uploads\n",
    "        }]\n",
    "    )\n",
    "\n",
    "    print(\"‚úÖ Liked videos, subscriptions, and total uploads stored in ChromaDB\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ab60f45c-f5ca-4005-9ec4-ca6f6e9f468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import chromadb\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def analyze_with_gpt():\n",
    "    # Load environment variables and OpenAI API key\n",
    "    load_dotenv()\n",
    "    openai_client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "    # Connect to ChromaDB\n",
    "    chroma_client = chromadb.PersistentClient(path=\"chroma_db\")\n",
    "    collection = chroma_client.get_collection(name=\"youtube_data\")\n",
    "\n",
    "    def fetch_data_from_chroma():\n",
    "        \"\"\"Retrieve liked videos, subscriptions, and uploads from ChromaDB.\"\"\"\n",
    "        liked_videos = collection.get(where={\"type\": \"liked_video\"})\n",
    "        subscriptions = collection.get(where={\"type\": \"subscription\"})\n",
    "        total_uploads = collection.get(where={\"type\": \"total_uploads\"})\n",
    "\n",
    "        return liked_videos, subscriptions, total_uploads\n",
    "\n",
    "    def analyze_liked_videos(liked_videos):\n",
    "        \"\"\"Analyze liked video titles and descriptions to find trends.\"\"\"\n",
    "        titles = [meta[\"title\"] for meta in liked_videos[\"metadatas\"]]\n",
    "        descriptions = [meta[\"description_chunk\"] for meta in liked_videos[\"metadatas\"]]\n",
    "\n",
    "        return {\n",
    "            \"titles\": titles[:20],  # Limit to avoid prompt overflow\n",
    "            \"descriptions\": descriptions[:20]\n",
    "        }\n",
    "\n",
    "    def analyze_categories(liked_videos):\n",
    "        \"\"\"Check how diverse the video categories are.\"\"\"\n",
    "        categories = [meta[\"category_id\"] for meta in liked_videos[\"metadatas\"]]\n",
    "        unique_categories = set(categories)\n",
    "\n",
    "        return {\n",
    "            \"total_categories\": len(unique_categories),\n",
    "            \"categories\": list(unique_categories)\n",
    "        }\n",
    "\n",
    "    def analyze_subscriptions(subscriptions):\n",
    "        \"\"\"Analyze the variety of subscriptions.\"\"\"\n",
    "        sub_titles = [meta[\"title\"] for meta in subscriptions[\"metadatas\"]]\n",
    "\n",
    "        return {\n",
    "            \"total_subscriptions\": len(sub_titles),\n",
    "            \"subscriptions\": sub_titles[:20]\n",
    "        }\n",
    "\n",
    "    def analyze_uploads(total_uploads):\n",
    "        \"\"\"Check the total number of videos uploaded.\"\"\"\n",
    "        upload_count = total_uploads[\"metadatas\"][0][\"count\"] if total_uploads[\"metadatas\"] else 0\n",
    "\n",
    "        return {\n",
    "            \"upload_count\": upload_count\n",
    "        }\n",
    "\n",
    "    def generate_humorous_analysis(liked_analysis, category_analysis, sub_analysis, upload_analysis):\n",
    "        \"\"\"Use GPT-4 to generate eight humorous insights and a final score.\"\"\"\n",
    "\n",
    "        system_prompt = \"\"\"\n",
    "        You are a humorous AI analyzing a person's YouTube activity.\n",
    "        Your job is to generate five funny insights about their YouTube habits.\n",
    "        Be witty, engaging, and creative in your analysis.\n",
    "\n",
    "        After the analysis, assign a **final score out of 100** based on these criteria:\n",
    "\n",
    "        1Ô∏è‚É£ **Uploads (20 points)**:\n",
    "           - 0 points: No uploads\n",
    "           - 5 points: 1-10 uploads\n",
    "           - 10 points: 10-50 uploads\n",
    "           - 20 points: More than 50 uploads\n",
    "\n",
    "        2Ô∏è‚É£ **Base Score (20 points)**:\n",
    "           - Every user gets a starting 20 points.\n",
    "\n",
    "        3Ô∏è‚É£ **Subscriptions Quality & Quantity (20 points)**:\n",
    "           - Higher score if they have a diverse and interesting set of subscriptions.\n",
    "\n",
    "        4Ô∏è‚É£ **Liked Videos Quality & Quantity (20 points)**:\n",
    "           - Higher score if they like a variety of high-quality videos.\n",
    "\n",
    "        5Ô∏è‚É£ **Match Between Liked Videos and Subscriptions (20 points)**:\n",
    "           - Higher score if the content they like matches what they subscribe to.\n",
    "\n",
    "        **Final Output Format:**\n",
    "        - eight humorous insights of about 20 words each\n",
    "        - Final score out of 100 with a funny remark about the score (just show the score and remark not do not explain the score)\n",
    "        - explanation for the score using the methodology given\n",
    "        \"\"\"\n",
    "\n",
    "        user_prompt = f\"\"\"\n",
    "        Here is the data about the user's YouTube activity:\n",
    "\n",
    "        1Ô∏è‚É£ **Liked Videos Analysis**:\n",
    "        - Titles: {json.dumps(liked_analysis['titles'], indent=2)}\n",
    "        - Descriptions: {json.dumps(liked_analysis['descriptions'], indent=2)}\n",
    "\n",
    "        2Ô∏è‚É£ **Category Analysis**:\n",
    "        - Unique Categories: {category_analysis['total_categories']}\n",
    "        - Category IDs: {category_analysis['categories']}\n",
    "\n",
    "        3Ô∏è‚É£ **Subscription Analysis**:\n",
    "        - Total Subscriptions: {sub_analysis['total_subscriptions']}\n",
    "        - Subscription Titles: {json.dumps(sub_analysis['subscriptions'], indent=2)}\n",
    "\n",
    "        4Ô∏è‚É£ **Uploads**:\n",
    "        - Total Videos Uploaded: {upload_analysis['upload_count']}\n",
    "\n",
    "        Generate eight humorous observations based on this data.\n",
    "        Then, assign a final **score out of 100** based on the scoring system.\n",
    "        \"\"\"\n",
    "\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    # **Main Execution**\n",
    "    # Fetch stored data\n",
    "    liked_videos, subscriptions, total_uploads = fetch_data_from_chroma()\n",
    "\n",
    "    # Analyze different aspects\n",
    "    liked_analysis = analyze_liked_videos(liked_videos)\n",
    "    category_analysis = analyze_categories(liked_videos)\n",
    "    sub_analysis = analyze_subscriptions(subscriptions)\n",
    "    upload_analysis = analyze_uploads(total_uploads)\n",
    "\n",
    "    # Generate humorous insights and score\n",
    "    humor_output = generate_humorous_analysis(liked_analysis, category_analysis, sub_analysis, upload_analysis)\n",
    "\n",
    "    # Create a structured result with comments, score, and explanation\n",
    "    # analysis_results = {\n",
    "    #     \"comments\": humor_output.split(\"\\n\\n\")[0],  # Assuming the first part is comments/insights\n",
    "    #     \"final_score\": humor_output.split(\"\\n\\n\")[1].split(\"\\n\")[0],  # The first line of the second part is the score\n",
    "    #     \"explanation\": \"\\n\".join(humor_output.split(\"\\n\\n\")[1:])  # The rest is the explanation\n",
    "    # }\n",
    "\n",
    "    return humor_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ef3fa20-a918-4bb7-bfbc-6f18dce2381b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7868\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7868/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=183975247830-bqgpkults0o7p6vhcj0us6v0durgqc8d.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.readonly&state=dEE3WxnDs5RJ8oaQYsslIcf5q4DALT&access_type=offline\n",
      "‚è≥ Time taken to fetch from ChromaDB: 0.00 seconds\n",
      "‚è≥ Time taken for GPT analysis: 24.61 seconds\n",
      "Please visit this URL to authorize this application: https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=183975247830-bqgpkults0o7p6vhcj0us6v0durgqc8d.apps.googleusercontent.com&redirect_uri=http%3A%2F%2Flocalhost%3A8080%2F&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fyoutube.readonly&state=rXUzslN6kEZaa4ytKTDc8wBxfFJAoX&access_type=offline\n",
      "‚è≥ Time taken to fetch from ChromaDB: 0.00 seconds\n",
      "‚è≥ Time taken for GPT analysis: 22.25 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import webbrowser\n",
    "# Global variable to store authenticated YouTube service\n",
    "youtube_service = None\n",
    "\n",
    "SCOPES = [\"https://www.googleapis.com/auth/youtube.readonly\"]\n",
    "\n",
    "# Authenticate and create YouTube service\n",
    "\n",
    "def authenticate():\n",
    "    \"\"\"Redirects user to Google Chrome for authentication and returns a success message.\"\"\"\n",
    "    global youtube_service\n",
    "    flow = InstalledAppFlow.from_client_secrets_file(\n",
    "        \"credentials.json\", SCOPES,\n",
    "        redirect_uri=\"http://localhost:8080/\"  # Explicitly set redirect URI\n",
    "    )\n",
    "    \n",
    "    # Force Chrome to open for authentication\n",
    "    chrome_path = \"C:/Program Files/Google/Chrome/Application/chrome.exe\" if os.name == \"nt\" else \"/usr/bin/google-chrome\"\n",
    "    os.environ[\"BROWSER\"] = chrome_path  # Set Chrome as the default browser\n",
    "\n",
    "    # Run authentication\n",
    "    creds = flow.run_local_server(port=8080, open_browser=True)\n",
    "\n",
    "    if not creds or not creds.valid:\n",
    "        return \"‚ùå Authentication failed! Please try again.\"\n",
    "    \n",
    "    youtube_service = build(\"youtube\", \"v3\", credentials=creds)  # Store in global variable\n",
    "    return \"‚úÖ Authentication successful! Click 'Analyze' to continue.\"\n",
    "\n",
    "\n",
    "# def authenticate():\n",
    "#     flow = InstalledAppFlow.from_client_secrets_file(\"credentials.json\", SCOPES)\n",
    "#     creds = flow.run_local_server(port=8080)\n",
    "#     print(\" theses are  \" )\n",
    "#     print(creds)\n",
    "#     youtube_service=build(\"youtube\", \"v3\", credentials=creds);\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "import time\n",
    "\n",
    "def analyze_youtube():\n",
    "    \"\"\"Fetches data from YouTube and analyzes it.\"\"\"\n",
    "    global youtube_service\n",
    "    if youtube_service is None:\n",
    "        return \"‚ùå Please login first!\"\n",
    "\n",
    "    status = gr.Markdown(\"‚úÖ Fetching stored data from ChromaDB...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    liked_videos = get_liked_videos(youtube_service)\n",
    "    subscriptions = get_subscriptions(youtube_service)\n",
    "    uploads = get_uploads(youtube_service)\n",
    "    \n",
    "    data = {\n",
    "        \"liked_videos\": liked_videos,\n",
    "        \"subscriptions\": subscriptions,\n",
    "        \"uploads\": uploads\n",
    "    }\n",
    "    \n",
    "    store_in_chromadb(data)\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"‚è≥ Time taken to fetch from ChromaDB: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    status.value = \"‚úÖ Data retrieved! Analyzing now...\"\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Process with GPT\n",
    "    start_time = time.time()\n",
    "    analysis_results = analyze_with_gpt()\n",
    "    end_time = time.time()\n",
    "    print(f\"‚è≥ Time taken for GPT analysis: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    status.value = \"‚úÖ Analysis complete! Displaying results...\"\n",
    "    return analysis_results\n",
    "\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "def display_results():\n",
    "    \"\"\"Triggers analysis and displays results one by one.\"\"\"\n",
    "    analysis_results = analyze_youtube()\n",
    "    \n",
    "    if isinstance(analysis_results, str):  # If error message\n",
    "        return analysis_results\n",
    "\n",
    "    comments = analysis_results\n",
    "   # final_score = analysis_results[\"final_score\"]\n",
    "    final_score=\"0\"\n",
    "    \n",
    "    output_text = \"\"\n",
    "    for comment in comments:\n",
    "        output_text += f\"**{comment}**\\n\\n\"\n",
    "        time.sleep(5)  # Wait 5 seconds before adding next comment\n",
    "    \n",
    "    output_text += f\"üèÜ **Your YouTube Score: {final_score} / 100 üéâ**\"\n",
    "    return output_text\n",
    "\n",
    "with gr.Blocks() as app:\n",
    "    gr.Markdown(\"# üé¨ YouTube Account Analyzer\")\n",
    "    gr.Markdown(\"Login with your YouTube account to analyze your activity!\")\n",
    "    \n",
    "    login_button = gr.Button(\"Login with YouTube\")\n",
    "    analyze_button = gr.Button(\"Analyze My Account\", visible=False)\n",
    "    \n",
    "    output = gr.Markdown(\"Waiting for login...\")\n",
    "    \n",
    "    # Login button triggers authentication\n",
    "    login_button.click(authenticate, outputs=output)\n",
    "\n",
    "    # Once authenticated, enable analysis button\n",
    "    login_button.click(lambda: gr.update(visible=True), outputs=analyze_button)\n",
    "\n",
    "    # Analyze button fetches data and processes it\n",
    "    analyze_button.click(display_results, outputs=output)\n",
    "\n",
    "app.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab83a8d-9489-4d4f-bb8d-efb6f1bcd606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
